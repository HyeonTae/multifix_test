{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "from torchtext.legacy.data import Dataset\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Predictor import Predictor\n",
    "from util.helpers import tokens_to_source, compilation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'DrRepair_deepfix'\n",
    "sync_pos = True\n",
    "use_with_sync_pos = False\n",
    "beam_size = 10\n",
    "iteration = 5\n",
    "max_seq_len = 400\n",
    "cuda = True\n",
    "add = False\n",
    "\n",
    "\n",
    "test_path = 'data/deepfix_raw_data'\n",
    "target_vocab_path = 'vocab/target_vocab.json'\n",
    "inverse_vocab_path = 'vocab/target_vocab_reverse.json'\n",
    "data = pickle.load(open('data/' + data_name + '.pkl', 'rb'))\n",
    "SRC, TRG = data['vocab']['src'], data['vocab']['trg']\n",
    "src_pad_idx = SRC.vocab.stoi[Constants.PAD_WORD]\n",
    "trg_pad_idx = TRG.vocab.stoi[Constants.PAD_WORD]\n",
    "trg_bos_idx = TRG.vocab.stoi[Constants.BOS_WORD]\n",
    "trg_eos_idx = TRG.vocab.stoi[Constants.EOS_WORD]\n",
    "\n",
    "insert_tok = list(range(1, 422))\n",
    "insert_tok = list(map(str,insert_tok))\n",
    "insert_idx = []\n",
    "for t in TRG.vocab.stoi.keys():\n",
    "    if t in insert_tok:\n",
    "        insert_idx.append(TRG.vocab.stoi[t])\n",
    "\n",
    "data_name = data_name + '_'\n",
    "if sync_pos:\n",
    "    if use_with_sync_pos:\n",
    "        data_type = 'use_with_sync_pos_' + ('cat_' if not add else '')\n",
    "    else:\n",
    "        data_type = 'sync_pos_' + ('cat_' if not add else '')\n",
    "else:\n",
    "    data_type = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(data_name, data_type, device):\n",
    "    checkpoint = torch.load('output/' + data_name + data_type + 'model.chkpt', map_location=device)\n",
    "    model_opt = checkpoint['settings']\n",
    "\n",
    "    model = Transformer(\n",
    "        model_opt.src_vocab_size,\n",
    "        model_opt.trg_vocab_size,\n",
    "\n",
    "        model_opt.src_pad_idx,\n",
    "        model_opt.trg_pad_idx,\n",
    "\n",
    "        trg_emb_prj_weight_sharing=model_opt.proj_share_weight,\n",
    "        emb_src_trg_weight_sharing=model_opt.embs_share_weight,\n",
    "        d_k=model_opt.d_k,\n",
    "        d_v=model_opt.d_v,\n",
    "        d_model=model_opt.d_model,\n",
    "        d_word_vec=model_opt.d_word_vec,\n",
    "        d_inner=model_opt.d_inner_hid,\n",
    "        n_layers=model_opt.n_layers,\n",
    "        n_head=model_opt.n_head,\n",
    "        dropout=model_opt.dropout,\n",
    "        sync_pos=model_opt.sync_pos,\n",
    "        use_with_sync_pos=model_opt.use_with_sync_pos,\n",
    "        add=add).to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    print('[Info] Trained model state loaded.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "predictor = Predictor(\n",
    "    model=load_model(data_name, data_type, device),\n",
    "    beam_size=beam_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    trg_bos_idx=trg_bos_idx,\n",
    "    trg_eos_idx=trg_eos_idx,\n",
    "    insert_idx=insert_idx,\n",
    "    device=device).to(device)\n",
    "\n",
    "unk_idx = SRC.vocab.stoi[SRC.unk_token]\n",
    "\n",
    "fig_path = 'result/'\n",
    "if not os.path.isdir(fig_path):\n",
    "    os.mkdir(fig_path)\n",
    "fig_path = fig_path + data_name + data_type + 'res/'\n",
    "if not os.path.isdir(fig_path):\n",
    "    os.mkdir(fig_path)\n",
    "database_path = fig_path\n",
    "database = database_path + \"result.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fix(program):\n",
    "    src_seq = [SRC.vocab.stoi.get(word, unk_idx) for word in program]\n",
    "    if sync_pos:\n",
    "        pred_seq = predictor.predict_sentence_with_sync_pos(torch.LongTensor([src_seq]).to(device))\n",
    "    else:\n",
    "        pred_seq = predictor.predict_sentence(torch.LongTensor([src_seq]).to(device))\n",
    "\n",
    "    pred_line = ' '.join(TRG.vocab.itos[idx] for idx in pred_seq)\n",
    "    pred_line = pred_line.replace(Constants.BOS_WORD, '').replace(Constants.EOS_WORD, '').replace(Constants.UNK_WORD, '0')\n",
    "\n",
    "    return pred_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = np.load(os.path.join(\n",
    "    test_path, 'test_raw.npy'), allow_pickle=True).item()\n",
    "\n",
    "tonum_data =  sum([len(test_dataset[pid]) for pid in test_dataset]) # Total number of data\n",
    "print(\"[Info] test_{} data length : {}\".format(data_type, tonum_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(database)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS programs (\n",
    "                prog_id text NOT NULL,\n",
    "                user_id text NOT NULL,\n",
    "                prob_id text NOT NULL,\n",
    "                code text NOT NULL,\n",
    "                name_dict text NOT NULL,\n",
    "                name_seq text NOT NULL,\n",
    "                PRIMARY KEY(prog_id)\n",
    "             )''')\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS iterations (\n",
    "                prog_id text NOT NULL,\n",
    "                iteration text NOT NULL,\n",
    "                network text NOT NULL,\n",
    "                fix text NOT NULL,\n",
    "                PRIMARY KEY(prog_id, iteration)\n",
    "             )''')\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS error_messages (\n",
    "                prog_id text NOT NULL,\n",
    "                iteration text NOT NULL,\n",
    "                network text NOT NULL,\n",
    "                error_message text NOT NULL,\n",
    "                FOREIGN KEY(prog_id, iteration, network) REFERENCES iterations(prog_id, iteration, network)\n",
    "             )''')\n",
    "\n",
    "sequences_of_programs = {}\n",
    "fixes_suggested_by_network = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_numbers(source):\n",
    "    lines = source.count('~')\n",
    "    for l in range(lines):\n",
    "        if l >= 10:\n",
    "            source = source.replace(list(str(l))[0] + \" \" + list(str(l))[1] + \" ~ \", \"\", 1)\n",
    "        else:\n",
    "            source = source.replace(str(l) + \" ~ \", \"\", 1)\n",
    "    source = source.replace(\"  \", \" \")\n",
    "    return source.split()\n",
    "\n",
    "with open(inverse_vocab_path, \"r\") as json_file:\n",
    "    inverse_vocab = json.load(json_file)\n",
    "\n",
    "with open(target_vocab_path, \"r\") as json_file:\n",
    "    target_vocab = json.load(json_file)\n",
    "\n",
    "def is_replace_edit(edit):\n",
    "    return str(edit) in target_vocab['replace'].values()\n",
    "\n",
    "def apply_edits(source, edits):\n",
    "    fixed = []\n",
    "    inserted = 0\n",
    "\n",
    "    for i, edit in enumerate(edits):\n",
    "        if i - inserted >= len(source):\n",
    "            break\n",
    "        if edit == '0':\n",
    "            fixed.append(source[i - inserted])\n",
    "        elif edit != '-1':\n",
    "            fixed.append(inverse_vocab[edit])\n",
    "            if not is_replace_edit(edits[i]):\n",
    "                inserted += 1\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem_id, test_programs in tqdm(test_dataset.items()):\n",
    "    sequences_of_programs[problem_id] = {}\n",
    "    fixes_suggested_by_network[problem_id] = {}\n",
    "\n",
    "    entries = []\n",
    "\n",
    "    for program, name_dict, name_sequence, user_id, program_id in test_programs:\n",
    "        sequences_of_programs[problem_id][program_id] = [program]\n",
    "        fixes_suggested_by_network[problem_id][program_id] = []\n",
    "        entries.append(\n",
    "            (program, name_dict, name_sequence, user_id, program_id,))\n",
    "\n",
    "        c.execute(\"INSERT OR IGNORE INTO programs VALUES (?,?,?,?,?,?)\", (program_id,\n",
    "                  user_id, problem_id, program, json.dumps(name_dict), json.dumps(name_sequence)))\n",
    "\n",
    "    for round_ in range(iteration):\n",
    "        to_delete = []\n",
    "        input_ = []\n",
    "\n",
    "        for i, entry in enumerate(entries):\n",
    "            _, _, _, _, program_id = entry\n",
    "\n",
    "            if sequences_of_programs[problem_id][program_id][-1] is not None:\n",
    "                tmp = sequences_of_programs[problem_id][program_id][-1]\n",
    "                input_.append(remove_line_numbers(tmp))\n",
    "            else:\n",
    "                to_delete.append(i)\n",
    "\n",
    "        to_delete = sorted(to_delete)[::-1]\n",
    "\n",
    "        for i in to_delete:\n",
    "            del entries[i]\n",
    "\n",
    "        assert len(input_) == len(entries)\n",
    "\n",
    "        if len(input_) == 0:\n",
    "            #print('Stopping before iteration %d (no programs remain)' % (round_ + 1))\n",
    "            break\n",
    "\n",
    "        cnt = 0\n",
    "        fixes = []\n",
    "        for i_program in input_:\n",
    "            fix = get_fix(i_program)\n",
    "            fixes.append(fix)\n",
    "            \n",
    "        to_delete = []\n",
    "\n",
    "        # Apply fixes\n",
    "        for i, entry, fix in zip(range(len(fixes)), entries, fixes):\n",
    "            _, _, _, _, program_id = entry\n",
    "            if sum(list(map(int, fix.split()))) == 0:\n",
    "                to_delete.append(i)\n",
    "            else:\n",
    "                program = apply_edits(remove_line_numbers(sequences_of_programs[problem_id][program_id][-1])\n",
    "                                      , fix.split())\n",
    "                sequences_of_programs[problem_id][program_id].append(\" \".join(program))\n",
    "\n",
    "                c.execute(\"INSERT OR IGNORE INTO iterations VALUES (?,?,?,?)\",\n",
    "                         (program_id, round_ + 1, data_name, fix))\n",
    "\n",
    "        to_delete = sorted(to_delete)[::-1]\n",
    "\n",
    "        for i in to_delete:\n",
    "            del entries[i]\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_results(database):\n",
    "    with sqlite3.connect(database) as conn:\n",
    "        c = conn.cursor()\n",
    "\n",
    "        error_counts = []\n",
    "\n",
    "        for row in c.execute(\"SELECT iteration, COUNT(*) FROM error_messages GROUP BY iteration ORDER BY iteration;\"):\n",
    "            error_counts.append(row[1])\n",
    "\n",
    "        query1 = \"\"\"SELECT COUNT(*)\n",
    "        FROM error_messages\n",
    "        WHERE iteration = 0 AND prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        for row in c.execute(query1):\n",
    "            initial_errors = row[0]\n",
    "\n",
    "        query2 = \"\"\"SELECT COUNT(*)\n",
    "        FROM error_messages\n",
    "        WHERE iteration = 10 AND prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        for row in c.execute(query2):\n",
    "            final_errors = row[0]\n",
    "\n",
    "        query3 = \"\"\"SELECT COUNT(DISTINCT prog_id)\n",
    "        FROM error_message_strings\n",
    "        WHERE iteration = 10 AND error_message_count = 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        query3_2 = \"\"\"SELECT DISTINCT prog_id\n",
    "        FROM error_message_strings\n",
    "        WHERE iteration = 10 AND error_message_count = 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        for row in c.execute(query3):\n",
    "            fully_fixed = row[0]\n",
    "\n",
    "        query4 = \"\"\"SELECT DISTINCT prog_id, error_message_count FROM error_message_strings\n",
    "        WHERE iteration = 0 AND error_message_count > 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        query5 = \"\"\"SELECT DISTINCT prog_id, error_message_count FROM error_message_strings\n",
    "        WHERE iteration = 10 AND error_message_count > 0 and prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"\"\"\n",
    "\n",
    "        original_errors = {}\n",
    "        for row in c.execute(query4):\n",
    "            original_errors[row[0]] = int(row[1])\n",
    "\n",
    "        partially_fixed = {}\n",
    "        unfixed = {}\n",
    "        for row in c.execute(query5):\n",
    "            if int(row[1]) < original_errors[row[0]]:\n",
    "                partially_fixed[row[0]] = int(row[1])\n",
    "            elif int(row[1]) == original_errors[row[0]]:\n",
    "                unfixed[row[0]] = int(row[1])\n",
    "            else:\n",
    "                print(row[0], row[1], original_errors[row[0]])\n",
    "\n",
    "        token_counts = []\n",
    "        assignments = None\n",
    "\n",
    "        for row in c.execute(\"SELECT COUNT(DISTINCT prob_id) FROM programs p WHERE prog_id NOT IN (SELECT p.prog_id FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count = 0);\"):\n",
    "            assignments = int(row[0])\n",
    "\n",
    "        for row in c.execute(\"SELECT code FROM programs p INNER JOIN error_message_strings e ON p.prog_id = e.prog_id WHERE e.iteration = 0 AND e.error_message_count <> 0;\"):\n",
    "            token_counts += [len(row[0].split())]\n",
    "\n",
    "        avg_token_count = np.mean(token_counts)\n",
    "\n",
    "        print(\"-------\")\n",
    "        print(\"Assignments: \", assignments)\n",
    "        print(\"Program count: \", len(token_counts))\n",
    "        print(\"Average token count: \", avg_token_count)\n",
    "        print(\"Error messages: \", initial_errors)\n",
    "        print(\"-------\")\n",
    "        print(\"Errors remaining: %d (%.1f\" % (final_errors,\n",
    "              final_errors/initial_errors*100) + \"%)\")\n",
    "        print(\"Reduction in errors: %d (%.1f\" % ((initial_errors - final_errors),\n",
    "              (initial_errors - final_errors)/initial_errors*100) + \"%)\")\n",
    "        print(\"Completely fixed programs: %d (%.1f\" % (fully_fixed,\n",
    "              fully_fixed/len(token_counts)*100) + \"%)\")\n",
    "        print(\"Partially fixed programs: %d (%.1f\" % (len(partially_fixed),\n",
    "              len(partially_fixed)/len(token_counts)*100) + \"%)\")\n",
    "        print(\"Unfixed programs: %d (%.1f\" % (len(unfixed),\n",
    "              len(unfixed)/len(token_counts)*100) + \"%)\")\n",
    "        print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_problem(problem_id):\n",
    "    global reconstruction, errors, errors_full, total_count, errors_test\n",
    "\n",
    "    c = conn.cursor()\n",
    "\n",
    "    reconstruction[problem_id] = {}\n",
    "    errors[problem_id] = {}\n",
    "    errors_full[problem_id] = {}\n",
    "    errors_test[problem_id] = []\n",
    "    candidate_programs = []\n",
    "\n",
    "    for row in c.execute('SELECT user_id, prog_id, code, name_dict, name_seq FROM programs WHERE prob_id = ?', (problem_id,)):\n",
    "        user_id, prog_id, initial = row[0], row[1], \" \".join(remove_line_numbers(row[2]))\n",
    "        name_dict = json.loads(row[3])\n",
    "        name_seq = json.loads(row[4])\n",
    "\n",
    "        candidate_programs.append(\n",
    "            (user_id, prog_id, initial, name_dict, name_seq,))\n",
    "\n",
    "    for _, prog_id, initial, name_dict, name_seq in candidate_programs:\n",
    "        #fixes_suggested_by_typo_network = []\n",
    "        #fixes_suggested_by_undeclared_network = []\n",
    "        fixes_suggested_by_network = []\n",
    "\n",
    "        #for row in c.execute('SELECT fix FROM iterations WHERE prog_id=? AND network = \\'typo\\' ORDER BY iteration', (prog_id,)):\n",
    "        #    fixes_suggested_by_typo_network.append(row[0])\n",
    "\n",
    "        #for row in c.execute('SELECT fix FROM iterations WHERE prog_id=? AND network = \\'ids\\' ORDER BY iteration', (prog_id,)):\n",
    "        #    fixes_suggested_by_undeclared_network.append(row[0])\n",
    "\n",
    "        for row in c.execute('SELECT fix FROM iterations WHERE prog_id=? ORDER BY iteration', (prog_id,)):\n",
    "            fixes_suggested_by_network.append(row[0])\n",
    "\n",
    "        reconstruction[problem_id][prog_id] = [initial]\n",
    "        temp_errors, temp_errors_full = compilation_errors(\n",
    "            tokens_to_source(initial, name_dict, False), database_path)\n",
    "        errors[problem_id][prog_id] = [temp_errors]\n",
    "        errors_full[problem_id][prog_id] = [temp_errors_full]\n",
    "\n",
    "        for fix in fixes_suggested_by_network:\n",
    "            temp_prog = \" \".join(apply_edits(\n",
    "                reconstruction[problem_id][prog_id][-1].split() , fix.split()))\n",
    "            temp_errors, temp_errors_full = compilation_errors(\n",
    "                tokens_to_source(temp_prog, name_dict, False), database_path)\n",
    "\n",
    "            if len(temp_errors) > len(errors[problem_id][prog_id][-1]):\n",
    "                break\n",
    "            else:\n",
    "                reconstruction[problem_id][prog_id].append(temp_prog)\n",
    "                errors[problem_id][prog_id].append(temp_errors)\n",
    "                errors_full[problem_id][prog_id].append(\n",
    "                    temp_errors_full)\n",
    "\n",
    "        while len(reconstruction[problem_id][prog_id]) <= 10:\n",
    "            reconstruction[problem_id][prog_id].append(\n",
    "                reconstruction[problem_id][prog_id][-1])\n",
    "            errors[problem_id][prog_id].append(errors[problem_id][prog_id][-1])\n",
    "            errors_full[problem_id][prog_id].append(\n",
    "                errors_full[problem_id][prog_id][-1])\n",
    "\n",
    "        errors_test[problem_id].append(errors[problem_id][prog_id])\n",
    "\n",
    "        for k, errors_t, errors_full_t in zip(range(len(errors[problem_id][prog_id])), errors[problem_id][prog_id], errors_full[problem_id][prog_id]):\n",
    "            c.execute(\"INSERT INTO error_message_strings VALUES(?, ?, ?, ?, ?)\", (\n",
    "                prog_id, k, 'typo', errors_full_t.decode('utf-8', 'ignore'), len(errors_t)))\n",
    "\n",
    "            for error_ in errors_t:\n",
    "                c.execute(\"INSERT INTO error_messages VALUES(?, ?, ?, ?)\",\n",
    "                            (prog_id, k, 'typo', error_.decode('utf-8', 'ignore'),))\n",
    "\n",
    "    count_t = len(candidate_programs)\n",
    "    total_count += count_t\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(database)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS error_message_strings (\n",
    "                prog_id text NOT NULL,\n",
    "                iteration text NOT NULL,\n",
    "                network text NOT NULL,\n",
    "                error_message_string text NOT NULL,\n",
    "                error_message_count integer NOT NULL,\n",
    "                FOREIGN KEY(prog_id, iteration, network) REFERENCES iterations(prog_id, iteration, network)\n",
    "             )''')\n",
    "\n",
    "problem_ids = []\n",
    "\n",
    "for row in c.execute('SELECT DISTINCT prob_id FROM programs'):\n",
    "    problem_ids.append(row[0])\n",
    "\n",
    "c.close()\n",
    "\n",
    "reconstruction = {}\n",
    "errors = {}\n",
    "errors_full = {}\n",
    "errors_test = {}\n",
    "\n",
    "fixes_per_stage = [0] * 10\n",
    "\n",
    "total_count = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for problem_id in tqdm(problem_ids):\n",
    "    do_problem(problem_id)\n",
    "\n",
    "time_t = time.time() - start\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print('Total time:', time_t, 'seconds')\n",
    "print('Total programs processed:', total_count)\n",
    "print('Average time per program:', int(float(time_t) / float(total_count) * 1000), 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fixes_num = {}\n",
    "errors_before = {}\n",
    "\n",
    "for problem_id in errors_test:\n",
    "    total_fixes_num[problem_id] = 0\n",
    "\n",
    "    for j, seq in enumerate(errors_test[problem_id]):\n",
    "        error_numbers = [len(x) for x in seq]\n",
    "        skip = False\n",
    "\n",
    "        for i in range(len(error_numbers) - 1):\n",
    "            assert (not error_numbers[i + 1] > error_numbers[i])\n",
    "            total_fixes_num[problem_id] += error_numbers[i] - \\\n",
    "                error_numbers[i + 1]\n",
    "\n",
    "            if error_numbers[i] != error_numbers[i + 1]:\n",
    "                fixes_per_stage[i] += error_numbers[i] - error_numbers[i + 1]\n",
    "\n",
    "total_numerator = 0\n",
    "total_denominator = 0\n",
    "\n",
    "for problem_id in errors_test:\n",
    "    total_numerator += total_fixes_num[problem_id]\n",
    "    total_denominator += sum([len(x[0]) for x in errors_test[problem_id]])\n",
    "\n",
    "\n",
    "print(int(float(total_numerator) * 100.0 / float(total_denominator)), '%')\n",
    "\n",
    "\n",
    "for stage in range(len(fixes_per_stage)):\n",
    "    print('Stage', stage, ':', fixes_per_stage[stage])\n",
    "\n",
    "get_final_results(database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
